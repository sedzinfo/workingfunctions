% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GLM_LINEAR_REGRESSION.R
\name{report_regression}
\alias{report_regression}
\title{Regression}
\usage{
report_regression(
  model,
  base_size = 10,
  title = "",
  file = NULL,
  w = 10,
  h = 10,
  plot_diagnostics = TRUE
)
}
\arguments{
\item{model}{object ml}

\item{base_size}{base font size}

\item{title}{plot title}

\item{file}{output filename}

\item{w}{width of pdf file. Relevant only when file string is not empty}

\item{h}{height of pdf file. Relevant only when file string is not empty}

\item{plot_diagnostics}{if TRUE it will output linear model diagnostics plots}
}
\description{
Regression
}
\note{
(1) Problematic values for standardized residuals > +-1.96 \cr
**Standardized residuals** are residuals divided by an estimated standard deviation and they can be interpreted as z scores in that: \cr
- 95.00% of z-scores lie between -1.96 and +1.96 \cr
- 99.00% of z-scores lie between -2.58 and +2.58 \cr 
- 99.99% of z-scores lie between -3.29 and +3.29 \cr
(2) **Studentized residuals** indicate the the ability of the model to predict that case. They follow a t distribution \cr
(3) **DFFits** indicate the difference between the adjusted predicted value and the original predicted value. 
Adjusted predicted value for a case refers to the predicted value of that case, when that case is excluded from model fit. \cr
(4) **Cook's distance** indicates leverage. Problematic values for cook's distance > 1 Cook and Weisberg (1982). \cr
(5) **Hat values** indicate leverage. Problematic values for Hat values 2 or 3 times the average (k+1/n) \cr
The average leverage value is defined as (k+1)/n, k=number of predictors, n=number of participants. 
Leverage values lie between 0 (no influence) and 1 (complete influence over prediction) \cr
- Hoaglin and Welsch (1978) recommends investigating cases with values greater than twice the average (2(k+1)/n) \cr
- Stevens (2002) recommends investigating cases with values greater than three times the average (3(k+1)/n) \cr
**T-tests** test the hypothesis that b's are different from 0 \cr
**Multiple R^2**: Variance Explained \cr
**Adjusted R^2**: Indicates how much variance in Y would be accounted for if the model is derived from the population from which the sample was taken. 
Idealy, R^2 = Adjusted R^2 \cr
**F-Statistic** tests the null hypothesis is that the overall model has no effect \cr
**Covariance ratios** critical values CVR>1+[3(k+1)/n] CRV<1-[3(k+1)/n]. In general we should obtain small values or we may have to remove cases\cr
**ASSUMPTIONS** \cr
(1) variable types: All predictors must be quantitative or categorical (with two levels), 
and the outcome variable must be quantitative (interval data), 
continuous and unbounded (no constraints on the variability of the outcome)
(2) Non-zero variance \cr
(3) No perfect multicollinearity \cr
(4) Predictors are uncorrelated with -external variables- \cr
(5) Homoscedasticity: At each level of the predictor variable(s), the variance of the residual terms should be constant. 
Residuals at each level of the predictor(s) should have similar variance (homoscedasticity) \cr
(6) Independent errors: For any two observations the residual terms should be uncorrelated (or independent)\cr
This eventuality is sometimes described as a lack of autocorrelation.
This assumption can be tested with the Durbin-Watson test,which tests for serial correlations between errors.
Specifically, it tests whether adjacent residuals are correlated
The size of the Durbin-Watson statistic depends upon the number of predictors in the model and the number of observations
As a very conservative rule of thumb, values less than 1 or greater than 3 are definitely cause for concern; 
however,values closer to 2 may still be problematic depending on your sample and model
R also provides a p-value of the autocorrelation. 
Be very careful with the Durbin-Watson test, though, as it depends on the order of the data: if you reorder your data, you-ll get a different value \cr
(7) Normally distributed errors: It is assumed that the residuals in the model are random, normally distributed variables with a mean of 0 \cr
(8) Independence: It is assumed that all of the values of the outcome variable are independent 
(in other words, each value of the outcome variable comes from a separate entity) \cr
(9) Linearity: The mean values of the outcome variable for each increment of the predictor(s) lie along a straight line \cr
}
\examples{
form<-formula(mpg~qsec)
regressionmodel<-lm(form,data=mtcars)
multipleregressionmodel<-lm(mpg~qsec*hp*wt*drat,data=mtcars)
res<-report_regression(model=regressionmodel,plot_diagnostics=TRUE)
res<-report_regression(model=multipleregressionmodel)
res<-report_regression(model=regressionmodel,file="regression")
res<-report_regression(model=multipleregressionmodel,
                       file="regression",
                       plot_diagnostics=TRUE)
}
\keyword{regression}
