---
title: "MNIST in Keras"
author: "Dimitrios Zacharatos"
output:
  html_document:
    css: styles.css
    toc: true
    toc_depth: 2
    theme: readable
vignette: >
  %\VignetteIndexEntry{MNIST in Keras}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r options,echo=FALSE,warning=FALSE,message=FALSE}
options(width=1000)
hook_output <- function(x, options) {
  paste0('<pre class="r-output">', knitr::knit_print(x), '</pre>')
}
knitr::knit_hooks$set(output = hook_output)
knitr::opts_chunk$set(echo=TRUE)
options(future.show.progress = FALSE)
```


```{r install_keras_tensorflow,echo=FALSE,warning=FALSE,message=FALSE}
# install.packages("remotes")
# remotes::install_github("rstudio/tensorflow")
# reticulate::install_python()
# library(tensorflow)
# tensorflow::install_tensorflow(envname = "r-tensorflow")
# install.packages("keras")
# library(keras)
# install_keras()
# reticulate::virtualenv_create("r-tensorflow")
# reticulate::use_virtualenv("r-tensorflow")
```


# Keras on the MNIST dataset
The code bellow works with the keras and psycholate library. Most of the code was taken from Chollet and Allaire (1917) book: Deep Learning with R.
```{r set-options}
library(keras)
options(width=1000)
options(digits=3)
options(scippen=999)
par(mai=c(.001,.001,.001,.001),mar=c(.001,.001,.001,.001),oma=c(.001,.001,.001,.001))
```

# Obtaining and preparing the data
The dataset is obtained from keras::dataset_mnist() function then usually we have to  
- Split the dataset in train and test datasets.  
- Scale the dataset (values in dataset should be -1 to 1 or 0 to 1).  
- Scaling on the test dataset should be done using the mean and sd of the training dataset.  
In this case, much of the aforementioned work is done already. In the mnist dataset, data are split into train and test, input and output. What is done bellow, is to simply rename the some variables to make our life a bit easier. The dataset consists of 60000 representations of train images and 10000 representations of test images and their corresponding labels.
```{r train_test,cache=TRUE}
mnist<-dataset_mnist()
tri<-train_images<-mnist$train$x
train_labels<-mnist$train$y
tei<-test_images<-mnist$test$x
test_labels<-mnist$test$y
```

## How the data look like
Each image is a 28*28 pixel grayscale of handwritten numbers, and their respective number values are stored in the labels datasets. Lets see the first image in the test dataset:
```{r shape,out.width="100%",cache=TRUE}
tei[1,,] # this is how the image looks as a matrix representation
plot(as.raster(tei[1,,],max=255)) # this is how the image looks like as a raster representation
test_labels[1] # and this is it's corresponding value
```

## Data Wrangling
Each image is a  28 by 28 matrix with values ranging from 0 to 255, thus the input of the network has to be 28*28=784 units. Inputs should be reshaped in a format acceptable by the network input layer. Input data should also be scaled by dividing by 255. This scaling results in a 0 - 1 range.
```{r wrangling,cache=TRUE}
train_images<-array_reshape(train_images,c(60000,28*28))
train_images<-train_images/255
test_images<-array_reshape(test_images,c(10000,28*28))
test_images<-test_images/255
train_labels<-to_categorical(train_labels)
test_labels<-to_categorical(test_labels)
```

# Network specification
The specified network for our task, is a simple sequential network with an input layer, two hidden layers, and an output layer of 10 units. Each unit in each layer connects with all other units in their neighboring layer. Each unit in the output layer represents 10 digits from 1 to 0 [1 2 3 4 5 6 7 8 9 0]. The measures employed to evaluate the network accuracy is the root mean square, categorical cross entropy and accuracy. The choice of these methods partly depends on the nature of the output. In our case the output units should return 0 or 1.
```{r spesification,cache=TRUE}
network<-keras_model_sequential() %>%
  layer_dense(units=100,activation="relu",input_shape=c(28*28)) %>%
  layer_dense(units=100,activation="relu") %>%
  layer_dense(units=10,activation="softmax")
network %>% compile(optimizer="rmsprop",
                    loss="categorical_crossentropy",
                    metrics=c("accuracy"))
network %>% fit(train_images,train_labels,epochs=10,batch_size=784,verbose=0)
```

# Network training
```{r training,cache=TRUE}
network %>% fit(train_images,train_labels,epochs=100,batch_size=784)
network %>% evaluate(test_images,test_labels)
```

# Network evaluation
In order to see how well the network performs, we use the test dataset. The output of the test data is in a matrix form since each output unit represents a digit, and we have to classify 10 digits, the matrix is a 10*10000 dataset. However, after a short transformation it is possible to obtain a vector of integers from 0 to 9.
```{r output_transformation,cache=TRUE}
rtl<-c()
for(i in 1:nrow(test_labels))
  rtl<-c(rtl,which(test_labels[i,]==1)-1)
summary(rtl)
deepviz::plot_model(network)
```

## Confusion matrix
An ideal model should produce a confusion matrix where all observed variables match the predicted variables. This is represented by having all high frequencies in the diagonal whilst all off diagonal values should ideally be 0. In fact, the confusion matrix indicates around 98% prediction accuracy from 10000 images.
```{r confusion_matrix,cache=TRUE}
op<-data.frame(predict=predict_classes(network,test_images),observe=rtl)
psycholatefunctions::confusion_matrix_percent(op$predict,op$observe)
```

## Scatterplot
Another descriptive representation can be done using a scatterplot. In that case, all observations (observed and predicted data) should fall in the regression line.
```{r,out.width="100%",results="hide",cache=TRUE}
psycholatefunctions::plot_scatterplot(op)
differences<-setdiff(1:length(op$predict),which(op$predict==op$observe))
```

## Errors
From the 10000 image test dataset, the network was unable to classify correctly `r length(differences)` images. It may be interesting to visualize these miss classified images.
```{r,out.width="10%"}
length(differences)
for(i in differences)
  plot(as.raster(tei[i,,],max=255))
```

